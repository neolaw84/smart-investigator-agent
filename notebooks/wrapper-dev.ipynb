{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "if os.getcwd().endswith(\"notebooks\"): os.chdir(\"..\")\n",
    "\n",
    "if \"./src\" not in sys.path: sys.path.append(\"./src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "from langgraph.types import Command\n",
    "\n",
    "from smart_investigator.foundation.agents.ticketing_agent_graph import create_ticketing_agent\n",
    "from smart_investigator.foundation.agents.travel_agent_graph import create_travel_agent\n",
    "from smart_investigator.foundation.agents.travel_agent_tool_agents import create_ticketing_agent_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticketing_llm = ChatOpenAI(\n",
    "  api_key=getenv(\"OPENAI_API_KEY\"),\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  model=\"gpt-4o-mini\",\n",
    ")\n",
    "\n",
    "user_inputs = [\n",
    "    \"I want to go to Bali, Indonesia. 2 tickets.\",\n",
    "    \"Sydney, Australia. Round-trip please. \",\n",
    "    \"yes. Proceed and invpoice me\",\n",
    "    \"...\",\n",
    "    \"yes, book the tickets for me.\",\n",
    "    \"exit\",\n",
    "    \"exit\"\n",
    "]\n",
    "input_iter = iter(user_inputs)\n",
    "\n",
    "greeting = \"Hi! I'm your ticketing assistant. How can I help you today?\\n\"\n",
    "print (greeting)\n",
    "config={\"configurable\": {\"thread_id\": str(uuid4()) }}\n",
    "agent_tkt = create_ticketing_agent(ticketing_llm, InMemorySaver())\n",
    "while True:    \n",
    "    content = next(input_iter) # input(\"Your response (or 'exit' to quit): \")\n",
    "    if content.lower() in ['exit', 'quit']:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    human_message = HumanMessage(content=content)\n",
    "    human_message.pretty_print()\n",
    "    response = agent_tkt.invoke({\"messages\": [human_message], \"loop_counter\": 0}, config=config)\n",
    "    while \"__interrupt__\" in response:\n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "        human_response = next(input_iter) # input (str(response[\"messages\"][-1].content) + \"\\nYour response: \")\n",
    "        response = agent_tkt.invoke(Command(resume=response), config=config)\n",
    "\n",
    "    response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = [\n",
    "    \"I want to go to Bali. For 2 adults. Next week Wednesday.\",\n",
    "    \"Departing from Sydney Australia, 30th November return. Nothing else. \",\n",
    "    \"yes, proceed with the booking\",\n",
    "    \"ok,\",\n",
    "    \"exit\",\n",
    "    \"exit\"\n",
    "]\n",
    "input_iter = iter(user_inputs)\n",
    "\n",
    "def llm_factory():\n",
    "    return ChatOpenAI(\n",
    "      api_key=getenv(\"OPENAI_API_KEY\"),\n",
    "      base_url=\"https://openrouter.ai/api/v1\",\n",
    "      model=\"gpt-4o-mini\",\n",
    "    )\n",
    "\n",
    "def checkpointer_factory():\n",
    "    return InMemorySaver()\n",
    "\n",
    "llm_travel = llm_factory()\n",
    "\n",
    "def travel_agent_tool_factory(llm_factory, checkpointer_factory):\n",
    "    agent_tkt = create_ticketing_agent(llm_factory(), checkpointer_factory())\n",
    "    return [create_ticketing_agent_tool(agent_tkt=agent_tkt)]\n",
    "\n",
    "agent_tools_travel = travel_agent_tool_factory(llm_factory, checkpointer_factory)\n",
    "\n",
    "greeting = \"Hi! I'm your travel assistant. How can I help you today?\\n\"\n",
    "print (greeting)\n",
    "config={\"configurable\": {\"thread_id\": str(uuid4()) }}\n",
    "agent = create_travel_agent(llm_travel, InMemorySaver(), agent_tools=agent_tools_travel)\n",
    "while True:    \n",
    "    content = next(input_iter) # input(\"Your response (or 'exit' to quit):\") # next(input_iter) # input(\"Your response (or 'exit' to quit): \")\n",
    "    if content.lower() in ['exit', 'quit']:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    human_message = HumanMessage(content=content)\n",
    "    human_message.pretty_print()\n",
    "    response = agent.invoke({\"messages\": [human_message], \"loop_counter\": 0}, config=config)\n",
    "    while \"__interrupt__\" in response:\n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "        print (f\"__interrupt__ : {response['__interrupt__']}\")\n",
    "        human_response = next(input_iter) # input (str(response[\"messages\"][-1].content) + \"\\nYour response: \")\n",
    "        human_message = HumanMessage(content=human_response)\n",
    "        human_message.pretty_print()\n",
    "        response = agent.invoke(Command(resume=response), config=config)\n",
    "\n",
    "    response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from os import getenv\n",
    "\n",
    "# MLflow imports required for the request/response objects\n",
    "from mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentResponse\n",
    "from mlflow.pyfunc import PythonModelContext\n",
    "from smart_investigator.foundation.agents.ticketing_agent import TicketingAgentWrapper\n",
    "\n",
    "# --- Setup from original script ---\n",
    "user_inputs = [\n",
    "    \"I want to go to Bali, Indonesia. 2 tickets.\",\n",
    "    \"Sydney, Australia. Round-trip please. \",\n",
    "    \"yes. Proceed and invoice me\",\n",
    "    \"yes, finalize the booking for me.\", \n",
    "    \"ok\",\n",
    "    \"exit\",\n",
    "    \"exit\"\n",
    "]\n",
    "input_iter = iter(user_inputs)\n",
    "\n",
    "# --- 1. Initialize the TicketingAgentWrapper ---\n",
    "print(\"Initializing agent wrapper...\")\n",
    "agent_wrapper = TicketingAgentWrapper()\n",
    "\n",
    "# Manually call load_context to initialize self.graph and self.llm\n",
    "# Our simulated _load_llm_from_context will use env vars\n",
    "# We pass `None` as the context, which our implementation handles.\n",
    "agent_wrapper.load_context(context=PythonModelContext(artifacts={}, model_config={}))\n",
    "print(\"Agent wrapper ready.\")\n",
    "\n",
    "# --- 2. Set up for ResponsesAgent interaction ---\n",
    "# This ID will be used for all requests in this conversation\n",
    "conversation_id = str(uuid.uuid4())\n",
    "\n",
    "# This flag tracks if the agent is waiting for a resume input\n",
    "is_waiting_for_resume = False\n",
    "\n",
    "greeting = \"Hi! I'm your ticketing assistant. How can I help you today?\\n\"\n",
    "print(greeting)\n",
    "\n",
    "# --- 3. Run the interaction loop ---\n",
    "while True:\n",
    "    try:\n",
    "        # Get the next user input\n",
    "        content = next(input_iter) \n",
    "        print(f\"\\nUser: {content}\")\n",
    "    except StopIteration:\n",
    "        print(\"\\n--- End of user inputs ---\")\n",
    "        break\n",
    "\n",
    "    # Check for exit condition\n",
    "    if content.lower() in ['exit', 'quit']:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # --- 4. Build the ResponsesAgentRequest ---\n",
    "    request_input = []\n",
    "    request_custom_inputs = {}\n",
    "\n",
    "    if is_waiting_for_resume:\n",
    "        # If we were interrupted, this input is the \"resume\" signal\n",
    "        # We serialize it to JSON as required by our design\n",
    "        request_custom_inputs[\"__resume__\"] = json.dumps(content)\n",
    "        # request.input is empty, as the payload is in custom_inputs\n",
    "    else:\n",
    "        # This is a new, normal message\n",
    "        request_input = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    # Create the request object\n",
    "    request = ResponsesAgentRequest(\n",
    "        input=request_input,\n",
    "        context={\"conversation_id\": conversation_id},\n",
    "        custom_inputs=request_custom_inputs\n",
    "    )\n",
    "\n",
    "    # --- 5. Call the wrapper's predict method ---\n",
    "    # This single call replaces the .invoke() and the inner interrupt loop\n",
    "    try:\n",
    "        response: ResponsesAgentResponse = agent_wrapper.predict(request)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during agent prediction: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # --- 6. Process the ResponsesAgentResponse ---\n",
    "    \n",
    "    # The agent's message (final or interrupt) is in the output\n",
    "    if response.output:\n",
    "        agent_message = response.output[0].content\n",
    "        print(f\"Agent: {agent_message}\")\n",
    "    else:\n",
    "        print(\"Agent: [No response output]\")\n",
    "\n",
    "    # --- 7. Update state for the next loop ---\n",
    "    if \"__interrupt__\" in response.custom_outputs:\n",
    "        # The agent has paused. The next input will be a resume.\n",
    "        print(\"[Agent is waiting for more information]\")\n",
    "        is_waiting_for_resume = True\n",
    "    else:\n",
    "        # The agent finished. The next input will be a new message.\n",
    "        is_waiting_for_resume = False\n",
    "        \n",
    "        # If the graph finishes without an interrupt *or* a final message,\n",
    "        # we can end the loop.\n",
    "        if not response.output:\n",
    "            print(\"[Agent finished processing]\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "from os import getenv\n",
    "\n",
    "# MLflow imports required for the request/response objects\n",
    "from mlflow.types.responses import ResponsesAgentRequest, ResponsesAgentResponse\n",
    "from mlflow.pyfunc import PythonModelContext\n",
    "from smart_investigator.foundation.agents.travel_agent import TravelAgentWrapper\n",
    "\n",
    "\n",
    "# --- 1. Initialize the TravelAgentWrapper ---\n",
    "print(\"Initializing agent wrapper...\")\n",
    "agent_wrapper = TravelAgentWrapper()\n",
    "\n",
    "# Manually call load_context to initialize self.graph and self.llm\n",
    "# Our simulated _load_llm_from_context will use env vars\n",
    "# We pass `None` as the context, which our implementation handles.\n",
    "agent_wrapper.load_context(context=PythonModelContext(artifacts={}, model_config={}))\n",
    "print(\"Agent wrapper ready.\")\n",
    "\n",
    "# --- 2. Set up for ResponsesAgent interaction ---\n",
    "# This ID will be used for all requests in this conversation\n",
    "conversation_id = str(uuid.uuid4())\n",
    "\n",
    "# This flag tracks if the agent is waiting for a resume input\n",
    "is_waiting_for_resume = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup from original script ---\n",
    "user_inputs = [\n",
    "    \"I want to go to Bali. For 2 adults. Next week Wednesday.\",\n",
    "    \"Departing from Sydney Australia, 30th November return. Nothing else. \",\n",
    "    \"yes, proceed with the booking\",\n",
    "    \"ok,\",\n",
    "    \"exit\",\n",
    "    \"exit\"\n",
    "]\n",
    "input_iter = iter(user_inputs)\n",
    "\n",
    "greeting = \"Hi! I'm your travel agent. How can I help you today?\\n\"\n",
    "print(greeting)\n",
    "\n",
    "# --- 3. Run the interaction loop ---\n",
    "while True:\n",
    "    try:\n",
    "        # Get the next user input\n",
    "        content = next(input_iter) \n",
    "        print(f\"\\nUser: {content}\")\n",
    "    except StopIteration:\n",
    "        print(\"\\n--- End of user inputs ---\")\n",
    "        break\n",
    "\n",
    "    # Check for exit condition\n",
    "    if content.lower() in ['exit', 'quit']:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # --- 4. Build the ResponsesAgentRequest ---\n",
    "    request_input = []\n",
    "    request_custom_inputs = {}\n",
    "\n",
    "    if is_waiting_for_resume:\n",
    "        # If we were interrupted, this input is the \"resume\" signal\n",
    "        # We serialize it to JSON as required by our design\n",
    "        request_custom_inputs[\"__resume__\"] = json.dumps(content)\n",
    "        # request.input is empty, as the payload is in custom_inputs\n",
    "    else:\n",
    "        # This is a new, normal message\n",
    "        request_input = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    # Create the request object\n",
    "    request = ResponsesAgentRequest(\n",
    "        input=request_input,\n",
    "        context={\"conversation_id\": conversation_id},\n",
    "        custom_inputs=request_custom_inputs\n",
    "    )\n",
    "\n",
    "    # --- 5. Call the wrapper's predict method ---\n",
    "    # This single call replaces the .invoke() and the inner interrupt loop\n",
    "    try:\n",
    "        response: ResponsesAgentResponse = agent_wrapper.predict(request)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during agent prediction: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # --- 6. Process the ResponsesAgentResponse ---\n",
    "    \n",
    "    # The agent's message (final or interrupt) is in the output\n",
    "    if response.output:\n",
    "        agent_message = response.output[0].content\n",
    "        print(f\"Agent: {agent_message}\")\n",
    "    else:\n",
    "        print(\"Agent: [No response output]\")\n",
    "\n",
    "    # --- 7. Update state for the next loop ---\n",
    "    if \"__interrupt__\" in response.custom_outputs:\n",
    "        # The agent has paused. The next input will be a resume.\n",
    "        print(\"[Agent is waiting for more information]\")\n",
    "        is_waiting_for_resume = True\n",
    "    else:\n",
    "        # The agent finished. The next input will be a new message.\n",
    "        is_waiting_for_resume = False\n",
    "        \n",
    "        # If the graph finishes without an interrupt *or* a final message,\n",
    "        # we can end the loop.\n",
    "        if not response.output:\n",
    "            print(\"[Agent finished processing]\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from pprint import pprint\n",
    "config={\"configurable\": {\"thread_id\": str(uuid4()) }}\n",
    "for chunk in agent_wrapper.graph.stream({\"messages\": [\"I want to go on a vacation. You should ask for help.\"], \"loop_counter\": -999}, config=config, stream_mode=\"values\"):\n",
    "    pprint(chunk)\n",
    "request = ResponsesAgentRequest(\n",
    "        input=[{\"role\": \"user\", \"content\": \"I want to go on a vacation. You should ask for help.\"}],\n",
    "        context={\"conversation_id\": str(uuid4())},\n",
    "        custom_inputs={}\n",
    "    )\n",
    "\n",
    "for chunk in agent_wrapper.predict_stream(request):\n",
    "    pprint(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
